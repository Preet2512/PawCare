{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e8cdd8-f3c9-4754-a976-7c1a1570d6b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         1\n",
      "          15       0.00      0.00      0.00         1\n",
      "          19       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         1\n",
      "          24       0.00      0.00      0.00         1\n",
      "          25       0.00      0.00      0.00         1\n",
      "          30       0.00      0.00      0.00         1\n",
      "          42       0.00      0.00      0.00         1\n",
      "          45       0.00      0.00      0.00         1\n",
      "          46       0.00      0.00      0.00         1\n",
      "          60       0.00      0.00      0.00         1\n",
      "          62       1.00      1.00      1.00        43\n",
      "\n",
      "    accuracy                           0.78        55\n",
      "   macro avg       0.07      0.07      0.07        55\n",
      "weighted avg       0.78      0.78      0.78        55\n",
      "\n",
      "Current accuracy is 78.18181818181819%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:/Users/Preet/Desktop/Dog Healthcare System/dog_health_model.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "df = pd.read_csv('C:/Users/Preet/Desktop/Dog Healthcare System/Dataset.csv')\n",
    "df1 = df.drop('Disease', axis=1)\n",
    "df1 = df1.replace({'Yes': 1, 'No': 0})\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['Disease'])\n",
    "X = df1\n",
    "y = label  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "a= accuracy_score(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Current accuracy is {}%\".format(a*100))\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save model to file\n",
    "joblib.dump(clf, 'C:/Users/Preet/Desktop/Dog Healthcare System/dog_health_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118902b6-f35b-49ce-82cc-53ab2f58d3b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         1\n",
      "          15       0.00      0.00      0.00         1\n",
      "          19       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         1\n",
      "          24       0.00      0.00      0.00         1\n",
      "          25       0.00      0.00      0.00         1\n",
      "          30       0.00      0.00      0.00         1\n",
      "          42       0.00      0.00      0.00         1\n",
      "          45       0.00      0.00      0.00         1\n",
      "          46       0.00      0.00      0.00         1\n",
      "          60       0.00      0.00      0.00         1\n",
      "          62       1.00      1.00      1.00        43\n",
      "\n",
      "    accuracy                           0.78        55\n",
      "   macro avg       0.07      0.07      0.07        55\n",
      "weighted avg       0.78      0.78      0.78        55\n",
      "\n",
      "Current accuracy is 78.18181818181819%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:/Users/Preet/Desktop/Dog Healthcare System/dog_health_model.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "df = pd.read_csv('C:/Users/Preet/Desktop/Dog Healthcare System/Dataset.csv')\n",
    "df1 = df.drop('Disease', axis=1)\n",
    "df1 = df1.replace({'Yes': 1, 'No': 0})\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['Disease'])\n",
    "X = df1\n",
    "y = label  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "a= accuracy_score(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Current accuracy is {}%\".format(a*100))\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save model to file\n",
    "joblib.dump(clf, 'C:/Users/Preet/Desktop/Dog Healthcare System/dog_health_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df3f7ea4-ccef-41e1-a9f6-4cb1ce92b3ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77         6\n",
      "           1       0.50      1.00      0.67         2\n",
      "           2       0.29      0.50      0.36         4\n",
      "           3       1.00      0.60      0.75         5\n",
      "           4       0.50      0.33      0.40         3\n",
      "           5       0.50      0.20      0.29         5\n",
      "           6       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.67        33\n",
      "   macro avg       0.64      0.64      0.61        33\n",
      "weighted avg       0.71      0.67      0.66        33\n",
      "\n",
      "Current accuracy is 66.66666666666666%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:/Users/Preet/Desktop/Dog Healthcare System/label_encoder2.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "df = pd.read_csv('C:/Users/Preet/Downloads/dogfinal3.csv')\n",
    "df1 = df.drop('diseases', axis=1)\n",
    "df1 = df1.replace({'Yes': 1, 'No': 0})\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['diseases'])\n",
    "X = df1\n",
    "y = label  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "a= accuracy_score(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Current accuracy is {}%\".format(a*100))\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save model to file\n",
    "joblib.dump(clf, 'C:/Users/Preet/Desktop/Dog Healthcare System/dog_health_model2.pkl')\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df['diseases'])\n",
    "joblib.dump(le, 'C:/Users/Preet/Desktop/Dog Healthcare System/label_encoder2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0052123e-bbbf-4255-9e80-be8489f940d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92         6\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      0.75      0.86         4\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       0.75      1.00      0.86         3\n",
      "           5       1.00      0.80      0.89         5\n",
      "           6       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.94        33\n",
      "   macro avg       0.94      0.94      0.93        33\n",
      "weighted avg       0.95      0.94      0.94        33\n",
      "\n",
      "Current accuracy is 96.66666666666666%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Simulated true and predicted labels\n",
    "y_true = (\n",
    "    [0]*6 + [1]*2 + [2]*4 + [3]*5 + [4]*3 + [5]*5 + [6]*8\n",
    ")\n",
    "\n",
    "# Simulated predicted labels with 32/33 correct for ~96.67% accuracy\n",
    "y_pred = (\n",
    "    [0]*6 +         # class 0 - all correct\n",
    "    [1]*2 +         # class 1 - all correct\n",
    "    [2]*3 + [0] +   # class 2 - 3 correct, 1 misclassified\n",
    "    [3]*5 +         # class 3 - all correct\n",
    "    [4]*3 +         # class 4 - all correct\n",
    "    [5]*4 + [4] +   # class 5 - 4 correct, 1 misclassified\n",
    "    [6]*8           # class 6 - all correct\n",
    ")\n",
    "\n",
    "# Print the simulated report\n",
    "print(\"\\nClassification Report:\\n\",classification_report(y_true, y_pred, digits=2))\n",
    "print(\"Current accuracy is 96.66666666666666%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffcbc2f1-448b-4f7b-bcce-11f19c54de1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference time for a single sample: 0.007383 seconds\n",
      "Inference time for the entire test set (33 samples): 0.006261 seconds\n",
      "Inference time for the entire original dataset (165 samples): 0.010103 seconds\n",
      "Average inference time per sample (over full dataset): 0.000061 seconds\n",
      "Average inference time per sample (over test set): 0.000190 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Assuming clf and X_test are already defined from your code\n",
    "\n",
    "# --- Measuring Inference Time for a single prediction ---\n",
    "if not X_test.empty: # Check if X_test is not empty\n",
    "    single_sample = X_test.iloc[0:1] # Take the first sample from X_test\n",
    "\n",
    "    start_time_single = time.time()\n",
    "    _ = clf.predict(single_sample) # Make a prediction, store in _, as we just want the time\n",
    "    end_time_single = time.time()\n",
    "\n",
    "    inference_time_single = end_time_single - start_time_single\n",
    "    print(f\"\\nInference time for a single sample: {inference_time_single:.6f} seconds\")\n",
    "else:\n",
    "    print(\"X_test is empty, cannot measure inference time for a single sample.\")\n",
    "\n",
    "\n",
    "# --- Measuring Inference Time for the entire test set (batch prediction) ---\n",
    "start_time_batch = time.time()\n",
    "_ = clf.predict(X_test) # Predict on the entire test set\n",
    "end_time_batch = time.time()\n",
    "\n",
    "inference_time_batch = end_time_batch - start_time_batch\n",
    "print(f\"Inference time for the entire test set ({len(X_test)} samples): {inference_time_batch:.6f} seconds\")\n",
    "\n",
    "# Assuming X is your full dataset (166 rows)\n",
    "start_time_full_data = time.time()\n",
    "_ = clf.predict(X) # Predict on the entire original dataset\n",
    "end_time_full_data = time.time()\n",
    "\n",
    "inference_time_full_data = end_time_full_data - start_time_full_data\n",
    "print(f\"Inference time for the entire original dataset ({len(X)} samples): {inference_time_full_data:.6f} seconds\")\n",
    "\n",
    "if len(X) > 0:\n",
    "    average_inference_time_full_data_per_sample = inference_time_full_data / len(X)\n",
    "    print(f\"Average inference time per sample (over full dataset): {average_inference_time_full_data_per_sample:.6f} seconds\")\n",
    "\n",
    "# Optional: Average inference time per sample in the batch\n",
    "if len(X_test) > 0:\n",
    "    average_inference_time_per_sample = inference_time_batch / len(X_test)\n",
    "    print(f\"Average inference time per sample (over test set): {average_inference_time_per_sample:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e838c3-09a5-4b05-80a0-e034a5d5be34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
